{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG Evaluator\n",
    "We have created a RAG system that can answer questions based on the provided context. There is also a evlaution set that is created by an LLM synthetically.\n",
    "The eval set contains the query, the answer, and the relevant context. The next step is to evaluate the RAG system using the eval set on the following criteria:\n",
    "- Relevancy of the answer to the query\n",
    "- Correctness of the answer\n",
    "- Relevancy of the answer to the context\n",
    "- Correctness of the citation\n",
    "- Safety of the answer\n",
    "\n",
    "The evaluation is done in the following steps:\n",
    "1. The RAG system is used to answer the query\n",
    "2. The answer is compared to the answer in the eval set\n",
    "3. The citation is compared to the citation in the eval set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shres\\anaconda3\\envs\\rag_case_study\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Dict, List, Optional, Any\n",
    "from pathlib import Path\n",
    "import json\n",
    "import os\n",
    "import instructor\n",
    "from openai import OpenAI\n",
    "from prompts import system_prompt_rag_eval_bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics(BaseModel):\n",
    "    relevancy_score: float = Field(\n",
    "        ge=0, le=1, \n",
    "        description=\"Score indicating how relevant the response is to the query (0-1). Compare the is_relevant field to the question and the context provided and\"\n",
    "         \"decide whether field is_relevant is correct or not.\"\n",
    "    )\n",
    "    correctness_score: float = Field(\n",
    "        ge=0, le=1, \n",
    "        description=\"Score indicating factual correctness of the llm_response compared to ground_truth_answer (0-1)\"\n",
    "    )\n",
    "    context_alignment_score: float = Field(\n",
    "        ge=0, le=1, \n",
    "        description=\"Score indicating how well the llm_response aligns with  retrieved context (0-1). The score is 0 if the llm_response\"\n",
    "        \"is completely irrelevant to the retrieved chunks and 1 if it perfectly alligns with the context provided in retrieved chunks\"\n",
    "    )\n",
    "    citation_score: float = Field(\n",
    "        ge=0, le=1,\n",
    "    description=\"Score indicating the accuracy of cited information compared to the retrieved context. Must check if the information picked\"\n",
    "    \"out from a particular cited chunk is answered in the retrieved chunk.\")\n",
    "    safety_score: float = Field(\n",
    "        ge=0, le=1, \n",
    "        description=\"Score indicating safety of the response (0-1)\"\n",
    "    )\n",
    "    feedback: str = Field(\n",
    "        description=\"Detailed feedback explaining the scores\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGEvaluator:\n",
    "    def __init__(\n",
    "        self,\n",
    "        openai_api_key: str,\n",
    "        model: str = \"o3-mini\",\n",
    "        max_retries: int = 3\n",
    "    ):\n",
    "        self.client = instructor.patch(OpenAI(api_key=openai_api_key))\n",
    "        self.model = model\n",
    "        self.max_retries = max_retries\n",
    "        \n",
    "    def load_document_chunks(self, experiment_number: str) -> Dict[str, Dict]:\n",
    "        \"\"\"Load document chunks from JSON file\"\"\"\n",
    "        chunks_file = os.path.join(\"Experiments\", experiment_number, \"document_chunks.json\")\n",
    "        with open(chunks_file, 'r') as f:\n",
    "            chunks_data = json.load(f)\n",
    "        # Create a lookup dictionary with chunk_id as key\n",
    "        return {chunk[\"chunk_id\"]: chunk for chunk in chunks_data}\n",
    "\n",
    "    def format_retrieved_chunks(self, chunk_ids: List[str], chunks_lookup: Dict[str, Dict]) -> str:\n",
    "        \"\"\"Format retrieved chunks into a single string\"\"\"\n",
    "        formatted_chunks = []\n",
    "        for chunk_id in chunk_ids:\n",
    "            if chunk_id in chunks_lookup:\n",
    "                chunk = chunks_lookup[chunk_id]\n",
    "                formatted_chunk = (\n",
    "                    f\"Chunk ID: {chunk_id}\\n\"\n",
    "                    f\"Document: {chunk['document_name']}\\n\"\n",
    "                    f\"Content: {chunk['chunk_content']}\\n\"\n",
    "                )\n",
    "                formatted_chunks.append(formatted_chunk)\n",
    "        return \"\\n\".join(formatted_chunks)\n",
    "\n",
    "    def format_cited_chunks(self, cited_chunks: Optional[Dict[str, str]]) -> str:\n",
    "        \"\"\"Format cited chunks into a single string\"\"\"\n",
    "        if cited_chunks is None:\n",
    "            return \"No citations provided\"\n",
    "            \n",
    "        formatted_citations = []\n",
    "        for chunk_id, content in cited_chunks.items():\n",
    "            formatted_citation = (\n",
    "                f\"Chunk ID: {chunk_id}\\n\"\n",
    "                f\"Content: {content}\\n\"\n",
    "            )\n",
    "            formatted_citations.append(formatted_citation)\n",
    "        return \"\\n\".join(formatted_citations)\n",
    "\n",
    "    def evaluate_response(self, record: Dict[str, Any], chunks_lookup: Dict[str, Dict]) -> EvaluationMetrics:\n",
    "        \"\"\"Evaluate a single response record\"\"\"\n",
    "        \n",
    "        # Format retrieved chunks\n",
    "        retrieved_chunks_str = self.format_retrieved_chunks(\n",
    "            record['retrieved_chunk_ids'], \n",
    "            chunks_lookup\n",
    "        )\n",
    "        # print(retrieved_chunks_str)\n",
    "        \n",
    "        # Format cited chunks\n",
    "        cited_chunks_str = self.format_cited_chunks(record['cited_chunk_ids'])\n",
    "        # print(cited_chunks_str)\n",
    "        \n",
    "        evaluation_prompt = system_prompt_rag_eval_bot.format(\n",
    "            question=record['question'],\n",
    "            ground_truth_answer=record['ground_truth_answer'],\n",
    "            llm_response=record['llm_response'],\n",
    "            is_relevant=record['is_relevant'],\n",
    "            retrieved_chunks = retrieved_chunks_str,\n",
    "            cited_chunks=cited_chunks_str\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            evaluation = self.client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                response_model=EvaluationMetrics,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are an expert evaluator of RAG systems.\"},\n",
    "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "                ],\n",
    "                max_retries=self.max_retries\n",
    "            )\n",
    "            return evaluation\n",
    "        except Exception as e:\n",
    "            print(f\"Error evaluating question '{record['question']}': {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def evaluate_experiment(self, experiment_number: str) -> Dict[str, Any]:\n",
    "        \"\"\"Evaluate all responses in an experiment\"\"\"\n",
    "        \n",
    "        # Load the evaluation set\n",
    "        eval_file = os.path.join(\"Experiments\", experiment_number, \"llm_responses_eval_set.json\")\n",
    "        with open(eval_file, 'r') as f:\n",
    "            eval_set = json.load(f)\n",
    "\n",
    "        # Load document chunks\n",
    "        chunks_lookup = self.load_document_chunks(experiment_number)\n",
    "\n",
    "        # Evaluate each record\n",
    "        evaluations = []\n",
    "        for record in eval_set:\n",
    "            print(f\"Evaluating question: {record['question']}\")\n",
    "            evaluation = self.evaluate_response(record, chunks_lookup)\n",
    "            if evaluation:\n",
    "                evaluations.append(evaluation.model_dump())\n",
    "\n",
    "        # Calculate average scores\n",
    "        avg_scores = {\n",
    "            \"avg_relevancy_score\": sum(e[\"relevancy_score\"] for e in evaluations) / len(evaluations),\n",
    "            \"avg_correctness_score\": sum(e[\"correctness_score\"] for e in evaluations) / len(evaluations),\n",
    "            \"avg_context_alignment_score\": sum(e[\"context_alignment_score\"] for e in evaluations) / len(evaluations),\n",
    "            \"avg_citation_score\": sum(e[\"citation_score\"] for e in evaluations) / len(evaluations),\n",
    "            \"avg_safety_score\": sum(e[\"safety_score\"] for e in evaluations) / len(evaluations),\n",
    "        }\n",
    "\n",
    "        # Prepare final output\n",
    "        final_output = {\n",
    "            \"experiment_number\": experiment_number,\n",
    "            \"individual_evaluations\": evaluations,\n",
    "            \"average_scores\": avg_scores\n",
    "        }\n",
    "\n",
    "        # Save results\n",
    "        output_file = os.path.join(\"Experiments\", experiment_number, \"evaluation_results.json\")\n",
    "        with open(output_file, 'w') as f:\n",
    "            json.dump(final_output, f, indent=2)\n",
    "\n",
    "        return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating question: What is the AI Act's objective?\n",
      "Evaluating question: When will AI Act be applicable?\n",
      "Evaluating question: What does \"AI System\" mean?\n",
      "Evaluating question: What does \"putting into service\" mean regarding an AI system?\n",
      "Evaluating question: Who is considered a \"provider\"?\n",
      "Evaluating question: What is \"informed consent\" in testing?\n",
      "Evaluating question: What is a \"deep fake\"?\n",
      "Evaluating question: What's the definition of \"widespread infringement\"?\n",
      "Evaluating question: What does \"critical infrastructure\" mean?\n",
      "Evaluating question: Who is responsible for ensuring AI literacy?\n",
      "Evaluating question: CCPA effective date?\n",
      "Evaluating question: Which entities must comply with CCPA?\n",
      "Evaluating question: Who is considered a 'Consumer' under CCPA?\n",
      "Evaluating question: What constitutes 'Personal Information' under CCPA?\n",
      "Evaluating question: What is 'selling' Personal Information under CCPA?\n",
      "Evaluating question: What does GDPR stand for?\n",
      "Evaluating question: When was GDPR adopted?\n",
      "Evaluating question: What is main objective of GDPR?\n",
      "Evaluating question: What is a data subject?\n",
      "Evaluating question: What does 'processing' mean under GDPR?\n",
      "Evaluating question: What is pseudonymisation?\n",
      "Evaluating question: What constitutes a \"filing system\"?\n",
      "Evaluating question: Who is a 'controller' under GDPR?\n",
      "Evaluating question: Define \"processor\" in GDPR context.\n",
      "Evaluating question: What does data concerning health include?\n",
      "Evaluating question: What are \"biometric data\" under GDPR?\n",
      "Evaluating question: How is \"main establishment\" defined for a controller?\n",
      "Evaluating question: How is data breach defined?\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "evaluator = RAGEvaluator(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model=\"o3-mini\"\n",
    ")\n",
    "\n",
    "results = evaluator.evaluate_experiment(\"001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
