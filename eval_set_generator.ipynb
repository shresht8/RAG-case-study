{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import (\n",
    "    FunctionDeclaration,\n",
    "    GenerativeModel,\n",
    "    Tool,\n",
    "    ToolConfig,\n",
    "    Part,\n",
    "    GenerationConfig,\n",
    ")\n",
    "PROJECT_ID = \"104916006626\"  # @param {type: \"string\", placeholder: \"[your-project-id]\" isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"xyz\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"australia-southeast1\")\n",
    "\n",
    "vertexai.init(project=\"104916006626\", location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Path to your service account key file\n",
    "key_path = \"C:\\\\Users\\\\shres\\\\Projects\\\\RAG-case-study\\keys\\\\keyproject-401005-6e1cdcbb5996.json\"\n",
    "\n",
    "# Create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path\n",
    ")\n",
    "\n",
    "# Set the credentials for the current environment\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "# auth_request = transport.requests.Request()\n",
    "# credentials.refresh(auth_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"difficulty\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"easy\", \"medium\", \"hard\"],\n",
    "            },\n",
    "            \"chunk_ids\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"description\": \"List of chunk IDs that the question and answer are based on\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\", \"difficulty\", \"chunk_ids\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import system_prompt_QA_eval_bot\n",
    "def generate_questions(context, num_questions=10):\n",
    "    \"\"\"\n",
    "    Generate a set of questions and answers from a given context.\n",
    "\n",
    "    Args:\n",
    "    context: The context to generate questions from.\n",
    "    num_questions: The number of questions to generate.\n",
    "\n",
    "    Returns:\n",
    "    A list of questions and answers.\n",
    "    \"\"\"\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "\n",
    "    response = model.generate_content(\n",
    "    system_prompt_QA_eval_bot.format(context=context, num_questions=num_questions),\n",
    "    generation_config=GenerationConfig(\n",
    "        response_mime_type=\"application/json\", response_schema=response_schema\n",
    "    ),\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a cross encode to get the similarity between the eval question and the document chunks\n",
    " The cross encoder will output a score between 0 and 1 for each question and document chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shres\\anaconda3\\envs\\rag_case_study\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shres\\.cache\\huggingface\\hub\\models--cross-encoder--stsb-distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "model = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n",
    "scores = model.predict([[\"My first\", \"sentence pair\"], [\"Second text\", \"pair\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11382268, 0.11522377], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shres\\anaconda3\\envs\\rag_case_study\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "from transformers import AutoTokenizer\n",
    "from docling.chunking import HybridChunker\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "class DocumentChunker:\n",
    "    def __init__(self, base_dir: str = \"processed_docs\", model_id: str = \"answerdotai/ModernBERT-base\"):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentChunker with necessary components.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory containing markdown files\n",
    "            model_id: Model ID for the tokenizer\n",
    "        \"\"\"\n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.model_id = model_id\n",
    "        \n",
    "        # Initialize components\n",
    "        self._setup_components()\n",
    "        \n",
    "        # Store results\n",
    "        self.document_chunks: Dict[str, List[str]] = {}\n",
    "\n",
    "    def _setup_components(self) -> None:\n",
    "        \"\"\"Initialize tokenizer, chunker and document converter.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.chunker = HybridChunker(\n",
    "            tokenizer=self.tokenizer,\n",
    "            merge_peers=True,\n",
    "        )\n",
    "        self.doc_converter = DocumentConverter()\n",
    "        \n",
    "    def process_single_document(self, file_path: Path) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process a single markdown file and return its chunks.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the markdown file\n",
    "            \n",
    "        Returns:\n",
    "            List of chunks for the document\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        try:\n",
    "            # Convert markdown to docling document\n",
    "            doc = self.doc_converter.convert(source=str(file_path)).document\n",
    "            \n",
    "            # Generate and store chunks in order\n",
    "            for chunk in self.chunker.chunk(dl_doc=doc):\n",
    "                chunks.append(self.chunker.serialize(chunk=chunk))\n",
    "                \n",
    "            self.logger.info(f\"Successfully processed {file_path.name} - Generated {len(chunks)} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def process_directory(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Process all markdown files in the directory and its subdirectories.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping document names to their ordered chunks\n",
    "        \"\"\"\n",
    "        # Find all markdown files\n",
    "        md_files = list(self.base_dir.glob(\"**/*-with-image-refs.md\"))\n",
    "        \n",
    "        if not md_files:\n",
    "            self.logger.warning(f\"No markdown files found in {self.base_dir}\")\n",
    "            return self.document_chunks\n",
    "        \n",
    "        self.logger.info(f\"Found {len(md_files)} markdown files to process\")\n",
    "        \n",
    "        # Process each file\n",
    "        for md_file in md_files:\n",
    "            self.logger.info(f\"Processing {md_file.relative_to(self.base_dir)}\")\n",
    "            \n",
    "            # Store chunks with document name as key\n",
    "            doc_key = md_file.stem\n",
    "            self.document_chunks[doc_key] = self.process_single_document(md_file)\n",
    "        \n",
    "        self.logger.info(f\"Completed processing all documents\")\n",
    "        return self.document_chunks\n",
    "    \n",
    "    def get_document_statistics(self) -> None:\n",
    "        \"\"\"Print statistics about processed documents and their chunks.\"\"\"\n",
    "        if not self.document_chunks:\n",
    "            print(\"No documents have been processed yet.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nDocument Processing Statistics:\")\n",
    "        print(\"-\" * 30)\n",
    "        for doc_name, chunks in self.document_chunks.items():\n",
    "            print(f\"\\nDocument: {doc_name}\")\n",
    "            print(f\"Number of chunks: {len(chunks)}\")\n",
    "            if chunks:\n",
    "                avg_chunk_length = sum(len(self.tokenizer.tokenize(chunk)) \n",
    "                                     for chunk in chunks) / len(chunks)\n",
    "                print(f\"Average chunk length: {avg_chunk_length:.2f} tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Found 3 markdown files to process\n",
      "INFO:__main__:Processing AI_ACT\\AI_ACT-with-image-refs.md\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.pipeline.base_pipeline:Processing document AI_ACT-with-image-refs.md\n",
      "INFO:docling.document_converter:Finished converting document AI_ACT-with-image-refs.md in 293.09 sec.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "INFO:__main__:Successfully processed AI_ACT-with-image-refs.md - Generated 152 chunks\n",
      "INFO:__main__:Processing Cybersecurity_California_Privacy\\Cybersecurity_California_Privacy-with-image-refs.md\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.pipeline.base_pipeline:Processing document Cybersecurity_California_Privacy-with-image-refs.md\n",
      "INFO:docling.document_converter:Finished converting document Cybersecurity_California_Privacy-with-image-refs.md in 17.14 sec.\n",
      "INFO:__main__:Successfully processed Cybersecurity_California_Privacy-with-image-refs.md - Generated 41 chunks\n",
      "INFO:__main__:Processing GDPR\\GDPR-with-image-refs.md\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.pipeline.base_pipeline:Processing document GDPR-with-image-refs.md\n",
      "INFO:docling.document_converter:Finished converting document GDPR-with-image-refs.md in 188.16 sec.\n",
      "INFO:__main__:Successfully processed GDPR-with-image-refs.md - Generated 122 chunks\n",
      "INFO:__main__:Completed processing all documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Processing Statistics:\n",
      "------------------------------\n",
      "\n",
      "Document: AI_ACT-with-image-refs\n",
      "Number of chunks: 152\n",
      "Average chunk length: 1133.82 tokens\n",
      "\n",
      "Document: Cybersecurity_California_Privacy-with-image-refs\n",
      "Number of chunks: 41\n",
      "Average chunk length: 266.54 tokens\n",
      "\n",
      "Document: GDPR-with-image-refs\n",
      "Number of chunks: 122\n",
      "Average chunk length: 938.01 tokens\n"
     ]
    }
   ],
   "source": [
    "doc_chunker = DocumentChunker()\n",
    "\n",
    "# Process all documents\n",
    "document_chunks = doc_chunker.process_directory()\n",
    "\n",
    "# Print statistics\n",
    "doc_chunker.get_document_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 315 chunks to document_chunks.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Prepare list to store all chunks with their metadata\n",
    "chunks_data = []\n",
    "\n",
    "# Loop through the document_chunks dictionary\n",
    "for doc_name, chunks in document_chunks.items():\n",
    "    # Process each chunk in the document\n",
    "    for i, chunk_content in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            \"document_name\": doc_name,\n",
    "            \"chunk_id\": f\"{doc_name}_chunk_{i}\",\n",
    "            \"chunk_content\": chunk_content\n",
    "        }\n",
    "        chunks_data.append(chunk_data)\n",
    "\n",
    "# Save to JSON file\n",
    "output_path = \"document_chunks.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(chunks_data)} chunks to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunks for 3 documents\n"
     ]
    }
   ],
   "source": [
    "# Load and restructure the chunks data\n",
    "with open(\"document_chunks.json\", 'r', encoding='utf-8') as f:\n",
    "    chunks_list = json.load(f)\n",
    "\n",
    "# Convert the flat list structure back to document_chunks dictionary\n",
    "document_chunks = {}\n",
    "for chunk in chunks_list:\n",
    "    doc_name = chunk['document_name']\n",
    "    if doc_name not in document_chunks:\n",
    "        document_chunks[doc_name] = []\n",
    "    document_chunks[doc_name].append(chunk['chunk_content'])\n",
    "\n",
    "print(f\"Loaded chunks for {len(document_chunks)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To verify all chunks are loaded correctly\n",
    "# len(document_chunks['GDPR-with-image-refs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_document_chunks(chunks_data: List[dict]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Format chunks from JSON data into strings organized by document.\n",
    "    \n",
    "    Args:\n",
    "        chunks_data: List of dictionaries containing chunk information from document_chunks.json\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping document names to their formatted content string\n",
    "    \"\"\"\n",
    "    formatted_docs = {}\n",
    "    \n",
    "    # Group chunks by document\n",
    "    for chunk in chunks_data:\n",
    "        doc_name = chunk['document_name']\n",
    "        \n",
    "        if doc_name not in formatted_docs:\n",
    "            formatted_docs[doc_name] = f\"{doc_name}:\\n\\n\"\n",
    "            \n",
    "        formatted_docs[doc_name] += \"----x----\\n\"\n",
    "        formatted_docs[doc_name] += f\"chunk_id: {chunk['chunk_id']}\\n\"\n",
    "        formatted_docs[doc_name] += f\"chunk_content: {chunk['chunk_content']}\\n\\n\"\n",
    "    \n",
    "    return formatted_docs\n",
    "\n",
    "# Load chunks from JSON\n",
    "with open(\"document_chunks.json\", 'r', encoding='utf-8') as f:\n",
    "    chunks_data = json.load(f)\n",
    "\n",
    "# Generate formatted documents\n",
    "formatted_docs = format_document_chunks(chunks_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"question\": \"What is the aim of the AI Act?\", \"answer\": \"The Artificial Intelligence Act aims to improve the functioning of the internal market by creating a uniform legal framework for the development, placing on the market, putting into service, and use of AI systems in the EU.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"What values does the AI Act adhere to?\", \"answer\": \"The AI Act is applied in accordance with Union values, including the protection of natural persons, undertakings, democracy, the rule of law, and environmental protection.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"How does the AI Act address market fragmentation?\", \"answer\": \"It ensures a consistent and high level of protection throughout the Union for trustworthy AI, preventing divergences that hinder free circulation, innovation, deployment, and uptake of AI systems in the internal market.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"What are some societal benefits of AI?\", \"answer\": \"AI contributes to economic, environmental, and societal benefits across industries and social activities by improving prediction, optimizing operations, allocating resources, and personalizing digital solutions.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"What kind of harm can AI cause?\", \"answer\": \"AI can cause material or immaterial harm, including physical, psychological, societal, or economic harm depending on its application, use, and level of technological development.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"How should AI be developed?\", \"answer\": \"AI should be developed in accordance with Union values and fundamental rights and freedoms, ensuring it is human-centric and serves to increase human well-being.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"What is the purpose of common rules for high-risk AI?\", \"answer\": \"The rules ensure a consistent and high level of protection of public interests (health, safety, fundamental rights) and are consistent with the Charter, non-discriminatory, and aligned with the Union\\'s international trade commitments.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"How does the AI Act support innovation?\", \"answer\": \"It fosters the development, use, and uptake of AI in the internal market with a high level of protection of public interests, supporting new innovative solutions and enabling a European ecosystem of public and private actors creating AI systems.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"What is the impact of the AI Act on existing Union law?\", \"answer\": \"The AI Act is complementary to existing Union law on data protection, consumer protection, fundamental rights, employment, worker protection, and product safety, leaving all rights and remedies unaffected.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"How does the AI Act relate to national labor law?\", \"answer\": \"The AI Act does not affect national labor law concerning employment and working conditions, including health and safety at work and the relationship between employers and workers.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"What rights do data subjects retain under the AI Act?\", \"answer\": \"Data subjects continue to enjoy all rights and guarantees awarded by Union law, including rights related to solely automated individual decision-making, including profiling.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"What is the definition of an \\'AI system\\'?\", \"answer\": \"An \\'AI system\\' is a machine-based system designed to operate with varying levels of autonomy, may exhibit adaptiveness after deployment, and infers how to generate outputs that can influence environments.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"Who is considered a \\'deployer\\'?\", \"answer\": \"A \\'deployer\\' is any natural or legal person, including public authorities, using an AI system under their authority, except for personal non-professional use.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"What constitutes \\'biometric identification\\'?\", \"answer\": \"It is the automated recognition of human features to establish identity by comparing biometric data to a reference database, regardless of consent.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"What is \\'biometric categorisation\\'?\", \"answer\": \"It is assigning natural persons to categories (sex, age, traits, etc.) based on biometric data, excluding ancillary features intrinsically linked to other commercial services.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"What is a \\'remote biometric identification system\\'?\", \"answer\": \"It\\'s an AI system for identifying individuals without their active involvement, typically at a distance, by comparing biometric data with a database.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"What is an \\'emotion recognition system\\'?\", \"answer\": \"An \\'emotion recognition system\\' is an AI system that identifies or infers emotions or intentions of natural persons from their biometric data.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"What constitutes a \\'publicly accessible space\\'?\", \"answer\": \"A \\'publicly accessible space\\' is any physical place accessible to an undetermined number of people, regardless of ownership or access conditions, but excludes prisons or border control.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"What is \\'AI literacy\\' in the context of this regulation?\", \"answer\": \"AI literacy equips providers, deployers, and affected persons with the knowledge to make informed decisions about AI systems and ensure compliance.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"Who does the AI Act apply to?\", \"answer\": \"The AI Act applies to providers, deployers, importers, distributors, manufacturers, authorized representatives, and affected persons related to AI systems within the EU.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_13\"]}, {\"question\": \"Are there exceptions to AI Act application for military use?\", \"answer\": \"Yes, AI systems exclusively for military, defense, or national security purposes are excluded, regardless of the entity using them.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_13\"]}, {\"question\": \"What is \\'post-market monitoring\\'?\", \"answer\": \"It refers to all the activities carried out by providers to collect and analyze data gained from real-world use of AI systems, with the goal of identifying necessary corrective or preventive actions.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"What are some manipulative AI practices that are prohibited?\", \"answer\": \"Using AI for subliminal manipulation, exploiting vulnerabilities, social scoring leading to harm, and predictive policing based solely on profiling are prohibited.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_17\"]}, {\"question\": \"When is real-time\\' remote biometric identification allowed?\", \"answer\": \"It is allowed for locating specific crime victims, preventing imminent threats to life or terrorist attacks, and identifying suspects of serious crimes, subject to authorization.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_17\"]}, {\"question\": \"What are some considerations for \\'real-time\\' biometric ID use?\", \"answer\": \"The nature of the situation, consequences for rights and freedoms, and compliance with safeguards (temporal, geographic, personal limitations) are crucial considerations.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_17\"]}, {\"question\": \"Who authorizes \\'real-time\\' biometric ID?\", \"answer\": \"A judicial or independent administrative authority of the Member State where the use is to take place.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_17\"]}, {\"question\": \"What if authorization for biometric ID is rejected?\", \"answer\": \"The use must stop immediately, and related data, results, and outputs deleted.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_17\"]}, {\"question\": \"When is an AI system considered high-risk?\", \"answer\": \"An AI system is high-risk if used as a safety component of a product requiring third-party conformity assessment under listed Union harmonization legislation or listed in Annex III and posing a significant risk.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_20\"]}, {\"question\": \"When is an AI system NOT considered high-risk despite being in Annex III?\", \"answer\": \"If it poses no significant risk to health, safety, or fundamental rights, such as by not materially influencing decision-making.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_20\"]}, {\"question\": \"What are the four conditions under which AI systems in Annex III are non-high-risk?\", \"answer\": \"Performing narrow procedural tasks, improving previously completed human activity, detecting decision-making patterns without replacing human assessment, or performing a task preparatory to an assessment.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_20\"]}, {\"question\": \"When is an AI system in Annex III always considered high-risk?\", \"answer\": \"When it performs profiling of natural persons.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_20\"]}, {\"question\": \"What are the key requirements for high-risk AI systems?\", \"answer\": \"Risk management, data quality, technical documentation, transparency, human oversight, robustness, accuracy, and cybersecurity.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_22\"]}, {\"question\": \"What does a risk management system entail for high-risk AI?\", \"answer\": \"A continuous process throughout the AI system\\'s lifecycle, including risk identification, analysis, evaluation, and mitigation.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_23\"]}, {\"question\": \"What are the quality criteria for data used in high-risk AI?\", \"answer\": \"The data must be relevant, sufficiently representative, error-free, complete, statistically appropriate, and address potential biases.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_24\"]}, {\"question\": \"When can special categories of personal data be processed for high-risk AI?\", \"answer\": \"Exceptionally, for bias detection and correction, if strictly necessary and subject to safeguards, when other data types are ineffective.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_24\"]}, {\"question\": \"What must technical documentation for high-risk AI include?\", \"answer\": \"It must demonstrate compliance with requirements and include information like system characteristics, algorithms, data, training processes, and risk management details.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_25\"]}, {\"question\": \"What is the purpose of record-keeping in high-risk AI?\", \"answer\": \"To ensure traceability, facilitate post-market monitoring, and monitor system operation, particularly for situations posing risks or substantial modifications.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_26\"]}, {\"question\": \"What transparency requirements apply to high-risk AI?\", \"answer\": \"The systems must be designed for transparent operation, enabling deployers to understand functionality and limitations via clear instructions for use.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_27\"]}, {\"question\": \"What information should instructions for use of high-risk AI contain?\", \"answer\": \"System characteristics, capabilities, limitations, risks to health and fundamental rights, human oversight measures, and resource requirements.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_27\"]}, {\"question\": \"What is the aim of human oversight in high-risk AI systems?\", \"answer\": \"To minimize or prevent risks to health, safety, or fundamental rights, even with other requirements in place, especially where risks persist.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_28\"]}, {\"question\": \"What are the requirements for accuracy and robustness in high-risk AI?\", \"answer\": \"High-risk AI must achieve and maintain appropriate levels of accuracy, robustness, and cybersecurity throughout its lifecycle.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_29\"]}, {\"question\": \"What is the role of cybersecurity in high-risk AI?\", \"answer\": \"To ensure resilience against unauthorized attempts to alter use, behavior, performance, or compromise security properties by exploiting vulnerabilities.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_29\"]}, {\"question\": \"What are the key obligations of providers of high-risk AI systems?\", \"answer\": \"Ensuring compliance with requirements, providing clear identification, establishing quality management, maintaining documentation, and undergoing conformity assessment.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_30\"]}, {\"question\": \"What does a quality management system for high-risk AI entail?\", \"answer\": \"Documented policies and procedures covering regulatory compliance, design, development, testing, data management, risk management, post-market monitoring, and accountability.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_31\"]}, {\"question\": \"What documentation must providers of high-risk AI keep?\", \"answer\": \"Technical documentation, quality management system documentation, records of notified body decisions, and the EU declaration of conformity.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_32\"]}, {\"question\": \"What are the obligations of importers of high-risk AI?\", \"answer\": \"Ensuring conformity with regulations, verifying documentation, indicating their details on the system, and maintaining storage conditions that don\\'t compromise compliance.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_37\"]}, {\"question\": \"What are the obligations of distributors of high-risk AI?\", \"answer\": \"Verifying CE marking, documentation, and compliance with provider obligations; taking corrective actions for non-compliant systems, and cooperating with authorities.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_38\"]}, {\"question\": \"When is a distributor considered a provider of high-risk AI?\", \"answer\": \"When putting their name on a system, making substantial modifications affecting compliance, or modifying the intended purpose to become high-risk.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_39\"]}]'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate eval set for one doc to see results\n",
    "generate_questions(formatted_docs['AI_ACT-with-image-refs'],50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate eval sets for each document\n",
    "eval_sets = {}\n",
    "for doc_id, formatted_content in formatted_docs.items():\n",
    "    print(f\"Generating questions for {doc_id}...\")\n",
    "    eval_sets[doc_id] = generate_questions(formatted_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Parse the eval sets and add document information\n",
    "parsed_eval_sets = []\n",
    "\n",
    "for doc_id, eval_set in eval_sets.items():\n",
    "    # Convert string response to Python list of dictionaries\n",
    "    questions = json.loads(eval_set)\n",
    "    \n",
    "    # Add document information to each question\n",
    "    for question in questions:\n",
    "        question['document'] = doc_id\n",
    "        parsed_eval_sets.append(question)\n",
    "\n",
    "# Save to JSON file\n",
    "output_path = \"evaluation_sets.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(parsed_eval_sets, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(parsed_eval_sets)} questions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eval sets when needed\n",
    "with open(\"evaluation_sets.json\", 'r', encoding='utf-8') as f:\n",
    "    eval_sets = json.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
