{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Evaluation Sets\n",
    "The idea behind creating these evaluation sets it to test the performance of the retriever in the RAG system. The issue lies in the fact that most of the times there is no ground truth to compare the performance of the retriever. This notebook explores the process of creating evaluation sets.\n",
    "These will be the steps followed to create the evaluation sets:\n",
    "1. The document chunker will be used to chunk the all the document into smaller chunks.\n",
    "2. The chunks are fed to the LLM to generate the evaluation sets which contain questions, answers, difficulty level and the chunk IDs that the question and answer are based on. The issue is that there is no ground truth to compare the performance of the LLM generating the evlaution set\n",
    "3. Since cross encoders capture deep relationships between the question and the chunk, we will use them to compare the performance of the LLM generating the evaluation set.\n",
    "4. The objective is to maximise the overlap between the synthetic evluation set generator and the cross encoder. The final ground truth will be the overlap between the evaluation set and the cross encoder. Another strategy is to use the combination of the synthetic LLM eval generator and the cross encoder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import (\n",
    "    FunctionDeclaration,\n",
    "    GenerativeModel,\n",
    "    Tool,\n",
    "    ToolConfig,\n",
    "    Part,\n",
    "    GenerationConfig,\n",
    ")\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "\n",
    "PROJECT_ID = \"104916006626\"  # @param {type: \"string\", placeholder: \"[your-project-id]\" isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"xyz\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"australia-southeast1\")\n",
    "\n",
    "vertexai.init(project=\"104916006626\", location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Path to your service account key file\n",
    "key_path = \"C:\\\\Users\\\\shres\\\\Projects\\\\RAG-case-study\\keys\\\\keyproject-401005-6e1cdcbb5996.json\"\n",
    "\n",
    "# Create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path\n",
    ")\n",
    "\n",
    "# Set the credentials for the current environment\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "# auth_request = transport.requests.Request()\n",
    "# credentials.refresh(auth_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"difficulty\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"easy\", \"medium\", \"hard\"],\n",
    "            },\n",
    "            \"chunk_ids\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                },\n",
    "                \"description\": \"List of chunk IDs that the question and answer are based on. List exactly 10 chunk ids\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\", \"difficulty\", \"chunk_ids\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import system_prompt_QA_eval_bot\n",
    "def generate_questions(context, num_questions=10):\n",
    "    \"\"\"\n",
    "    Generate a set of questions and answers from a given context.\n",
    "\n",
    "    Args:\n",
    "    context: The context to generate questions from.\n",
    "    num_questions: The number of questions to generate.\n",
    "\n",
    "    Returns:\n",
    "    A list of questions and answers.\n",
    "    \"\"\"\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "\n",
    "    response = model.generate_content(\n",
    "    system_prompt_QA_eval_bot.format(chunk_set=context, num_questions=num_questions),\n",
    "    generation_config=GenerationConfig(\n",
    "        response_mime_type=\"application/json\", response_schema=response_schema\n",
    "    ),\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a cross encode to get the similarity between the eval question and the document chunks\n",
    " The cross encoder will output a score between 0 and 1 for each question and document chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shres\\anaconda3\\envs\\rag_case_study\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shres\\.cache\\huggingface\\hub\\models--cross-encoder--stsb-distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "model = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n",
    "scores = model.predict([[\"My first\", \"sentence pair\"], [\"Second text\", \"pair\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11382268, 0.11522377], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shres\\anaconda3\\envs\\rag_case_study\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer\n",
    "from docling.chunking import HybridChunker\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "class DocumentChunker:\n",
    "    def __init__(self, base_dir: str = \"processed_docs\", model_id: str = \"answerdotai/ModernBERT-base\"):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentChunker with necessary components.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory containing markdown files\n",
    "            model_id: Model ID for the tokenizer\n",
    "        \"\"\"\n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.model_id = model_id\n",
    "        \n",
    "        # Initialize components\n",
    "        self._setup_components()\n",
    "        \n",
    "        # Store results\n",
    "        self.document_chunks: Dict[str, List[str]] = {}\n",
    "\n",
    "    def _setup_components(self) -> None:\n",
    "        \"\"\"Initialize tokenizer, chunker and document converter.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.chunker = HybridChunker(\n",
    "            tokenizer=self.tokenizer,\n",
    "            merge_peers=True,\n",
    "        )\n",
    "        self.doc_converter = DocumentConverter()\n",
    "        \n",
    "    def process_single_document(self, file_path: Path) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process a single markdown file and return its chunks.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the markdown file\n",
    "            \n",
    "        Returns:\n",
    "            List of chunks for the document\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        try:\n",
    "            # Convert markdown to docling document\n",
    "            doc = self.doc_converter.convert(source=str(file_path)).document\n",
    "            \n",
    "            # Generate and store chunks in order\n",
    "            for chunk in self.chunker.chunk(dl_doc=doc):\n",
    "                chunks.append(self.chunker.serialize(chunk=chunk))\n",
    "                \n",
    "            self.logger.info(f\"Successfully processed {file_path.name} - Generated {len(chunks)} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def process_directory(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Process all markdown files in the directory and its subdirectories.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping document names to their ordered chunks\n",
    "        \"\"\"\n",
    "        # Find all markdown files\n",
    "        md_files = list(self.base_dir.glob(\"**/*-with-image-refs.md\"))\n",
    "        \n",
    "        if not md_files:\n",
    "            self.logger.warning(f\"No markdown files found in {self.base_dir}\")\n",
    "            return self.document_chunks\n",
    "        \n",
    "        self.logger.info(f\"Found {len(md_files)} markdown files to process\")\n",
    "        \n",
    "        # Process each file\n",
    "        for md_file in md_files:\n",
    "            self.logger.info(f\"Processing {md_file.relative_to(self.base_dir)}\")\n",
    "            \n",
    "            # Store chunks with document name as key\n",
    "            doc_key = md_file.stem\n",
    "            self.document_chunks[doc_key] = self.process_single_document(md_file)\n",
    "        \n",
    "        self.logger.info(f\"Completed processing all documents\")\n",
    "        return self.document_chunks\n",
    "    \n",
    "    def get_document_statistics(self) -> None:\n",
    "        \"\"\"Print statistics about processed documents and their chunks.\"\"\"\n",
    "        if not self.document_chunks:\n",
    "            print(\"No documents have been processed yet.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nDocument Processing Statistics:\")\n",
    "        print(\"-\" * 30)\n",
    "        for doc_name, chunks in self.document_chunks.items():\n",
    "            print(f\"\\nDocument: {doc_name}\")\n",
    "            print(f\"Number of chunks: {len(chunks)}\")\n",
    "            if chunks:\n",
    "                avg_chunk_length = sum(len(self.tokenizer.tokenize(chunk)) \n",
    "                                     for chunk in chunks) / len(chunks)\n",
    "                print(f\"Average chunk length: {avg_chunk_length:.2f} tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Found 3 markdown files to process\n",
      "INFO:__main__:Processing AI_ACT\\AI_ACT-with-image-refs.md\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.pipeline.base_pipeline:Processing document AI_ACT-with-image-refs.md\n",
      "INFO:docling.document_converter:Finished converting document AI_ACT-with-image-refs.md in 293.09 sec.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (8230 > 8192). Running this sequence through the model will result in indexing errors\n",
      "INFO:__main__:Successfully processed AI_ACT-with-image-refs.md - Generated 152 chunks\n",
      "INFO:__main__:Processing Cybersecurity_California_Privacy\\Cybersecurity_California_Privacy-with-image-refs.md\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.pipeline.base_pipeline:Processing document Cybersecurity_California_Privacy-with-image-refs.md\n",
      "INFO:docling.document_converter:Finished converting document Cybersecurity_California_Privacy-with-image-refs.md in 17.14 sec.\n",
      "INFO:__main__:Successfully processed Cybersecurity_California_Privacy-with-image-refs.md - Generated 41 chunks\n",
      "INFO:__main__:Processing GDPR\\GDPR-with-image-refs.md\n",
      "INFO:docling.document_converter:Going to convert document batch...\n",
      "INFO:docling.pipeline.base_pipeline:Processing document GDPR-with-image-refs.md\n",
      "INFO:docling.document_converter:Finished converting document GDPR-with-image-refs.md in 188.16 sec.\n",
      "INFO:__main__:Successfully processed GDPR-with-image-refs.md - Generated 122 chunks\n",
      "INFO:__main__:Completed processing all documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document Processing Statistics:\n",
      "------------------------------\n",
      "\n",
      "Document: AI_ACT-with-image-refs\n",
      "Number of chunks: 152\n",
      "Average chunk length: 1133.82 tokens\n",
      "\n",
      "Document: Cybersecurity_California_Privacy-with-image-refs\n",
      "Number of chunks: 41\n",
      "Average chunk length: 266.54 tokens\n",
      "\n",
      "Document: GDPR-with-image-refs\n",
      "Number of chunks: 122\n",
      "Average chunk length: 938.01 tokens\n"
     ]
    }
   ],
   "source": [
    "doc_chunker = DocumentChunker()\n",
    "\n",
    "# Process all documents\n",
    "document_chunks = doc_chunker.process_directory()\n",
    "\n",
    "# Print statistics\n",
    "doc_chunker.get_document_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 315 chunks to document_chunks.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Prepare list to store all chunks with their metadata\n",
    "chunks_data = []\n",
    "\n",
    "# Loop through the document_chunks dictionary\n",
    "for doc_name, chunks in document_chunks.items():\n",
    "    # Process each chunk in the document\n",
    "    for i, chunk_content in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            \"document_name\": doc_name,\n",
    "            \"chunk_id\": f\"{doc_name}_chunk_{i}\",\n",
    "            \"chunk_content\": chunk_content\n",
    "        }\n",
    "        chunks_data.append(chunk_data)\n",
    "\n",
    "# Save to JSON file\n",
    "output_path = \"document_chunks.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(chunks_data)} chunks to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded chunks for 3 documents\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# If chunks are already generated, start here\n",
    "# Load and restructure the chunks data\n",
    "with open(\"document_chunks.json\", 'r', encoding='utf-8') as f:\n",
    "    chunks_list = json.load(f)\n",
    "\n",
    "# Convert the flat list structure back to document_chunks dictionary\n",
    "document_chunks = {}\n",
    "for chunk in chunks_list:\n",
    "    doc_name = chunk['document_name']\n",
    "    if doc_name not in document_chunks:\n",
    "        document_chunks[doc_name] = []\n",
    "    document_chunks[doc_name].append(chunk['chunk_content'])\n",
    "\n",
    "print(f\"Loaded chunks for {len(document_chunks)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To verify all chunks are loaded correctly\n",
    "# len(document_chunks['GDPR-with-image-refs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "def format_document_chunks(chunks_data: List[dict]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Format chunks from JSON data into strings organized by document.\n",
    "    \n",
    "    Args:\n",
    "        chunks_data: List of dictionaries containing chunk information from document_chunks.json\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping document names to their formatted content string\n",
    "    \"\"\"\n",
    "    formatted_docs = {}\n",
    "    \n",
    "    # Group chunks by document\n",
    "    for chunk in chunks_data:\n",
    "        doc_name = chunk['document_name']\n",
    "        \n",
    "        if doc_name not in formatted_docs:\n",
    "            formatted_docs[doc_name] = f\"{doc_name}:\\n\\n\"\n",
    "            \n",
    "        formatted_docs[doc_name] += \"----x----\\n\"\n",
    "        formatted_docs[doc_name] += f\"chunk_id: {chunk['chunk_id']}\\n\"\n",
    "        formatted_docs[doc_name] += f\"chunk_content: {chunk['chunk_content']}\\n\\n\"\n",
    "    \n",
    "    return formatted_docs\n",
    "\n",
    "# Load chunks from JSON\n",
    "with open(\"document_chunks.json\", 'r', encoding='utf-8') as f:\n",
    "    chunks_data = json.load(f)\n",
    "\n",
    "# Generate formatted documents\n",
    "formatted_docs = format_document_chunks(chunks_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[{\"question\": \"What is the purpose of the AI Act?\", \"answer\": \"The purpose of the AI Act is to improve the functioning of the internal market by laying down a uniform legal framework for the development, placing on the market, putting into service, and use of artificial intelligence systems in the Union.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_3\"]}, {\"question\": \"When did the European Parliament and Council adopt the AI Act?\", \"answer\": \"13 June 2024\", \"difficulty\": \"easy\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_1\", \"AI_ACT-with-image-refs_chunk_130\"]}, {\"question\": \"What does the AI Act lay down?\", \"answer\": \"The AI Act lays down harmonized rules for AI systems, prohibitions of certain AI practices, requirements for high-risk AI systems, transparency rules, rules for general-purpose AI models, rules on market monitoring and governance, and measures to support innovation.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_12\"]}, {\"question\": \"What is an \\'AI system\\'?\", \"answer\": \"A machine-based system designed to operate with varying levels of autonomy, may exhibit adaptiveness after deployment, and for explicit or implicit objectives, infers how to generate outputs influencing physical or virtual environments.\", \"difficulty\": \"hard\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_14\"]}, {\"question\": \"What are some prohibited AI practices?\", \"answer\": \"Some prohibited practices include manipulative or exploitative AI systems causing harm, social scoring leading to detrimental treatment, real-time remote biometric identification in public for law enforcement (with exceptions), and creating facial recognition databases via untargeted scraping.\", \"difficulty\": \"hard\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_17\"]}, {\"question\": \"What are some of the criteria for high-risk AI systems?\", \"answer\": \"High-risk AI systems are those intended to be used as safety components of products requiring third-party conformity assessments, and systems listed in Annex III that pose a significant risk of harm to health, safety, or fundamental rights.\", \"difficulty\": \"hard\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_20\"]}, {\"question\": \"What are some obligations of providers of high-risk AI systems?\", \"answer\": \"Providers must ensure compliance with requirements, affix CE marking, establish quality management and post-market monitoring systems, maintain technical documentation, and cooperate with authorities.\", \"difficulty\": \"hard\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_30\"]}, {\"question\": \"What is the role of the AI Office?\", \"answer\": \"The AI Office develops Union expertise and capabilities in AI and contributes to implementing, monitoring, and supervising Union law on AI.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_79\"]}, {\"question\": \"What are the rights of affected persons regarding AI decisions?\", \"answer\": \"Affected persons have the right to obtain explanations for decisions based on high-risk AI systems that produce legal effects or significantly affect them.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_102\"]}, {\"question\": \"When does the AI Act apply from?\", \"answer\": \"The AI Act applies from 2 August 2026, with some provisions, such as prohibitions and general provisions, applying from 2 February 2025.\", \"difficulty\": \"medium\", \"chunk_ids\": [\"AI_ACT-with-image-refs_chunk_130\"]}]'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate eval set for one doc to see results\n",
    "# generate_questions(formatted_docs['AI_ACT-with-image-refs'],10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating questions for AI_ACT-with-image-refs...\n",
      "Generating questions for Cybersecurity_California_Privacy-with-image-refs...\n",
      "Generating questions for GDPR-with-image-refs...\n"
     ]
    }
   ],
   "source": [
    "# Generate eval sets for each document\n",
    "eval_sets = {}\n",
    "for doc_id, formatted_content in formatted_docs.items():\n",
    "    print(f\"Generating questions for {doc_id}...\")\n",
    "    eval_sets[doc_id] = generate_questions(formatted_content,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of response: <class 'str'>\n",
      "\n",
      "First 200 characters of response:\n",
      "[{\"question\": \"What is the AI Act's main objective?\", \"answer\": \"The purpose of this Regulation is to improve the functioning of the internal market by laying down a uniform legal framework in particu\n",
      "\n",
      "JSON parsing error: Unterminated string starting at: line 1 column 24610 (char 24609)\n",
      "\n",
      "Problematic section:\n",
      "EU database for high-risk AI systems?\", \"answer\": \"The EU database contains information on high-risk\n"
     ]
    }
   ],
   "source": [
    "# # Let's first examine what we're getting\n",
    "# print(\"Type of response:\", type(eval_sets['AI_ACT-with-image-refs']))\n",
    "# print(\"\\nFirst 200 characters of response:\")\n",
    "# print(eval_sets['AI_ACT-with-image-refs'][:200])\n",
    "\n",
    "# # Try parsing with error handling\n",
    "# try:\n",
    "#     parsed_response = json.loads(eval_sets['AI_ACT-with-image-refs'])\n",
    "#     print(\"\\nSuccessfully parsed JSON!\")\n",
    "# except json.JSONDecodeError as e:\n",
    "#     print(f\"\\nJSON parsing error: {str(e)}\")\n",
    "#     # Print the problematic section of the string\n",
    "#     error_position = e.pos\n",
    "#     print(\"\\nProblematic section:\")\n",
    "#     print(eval_sets['AI_ACT-with-image-refs'][error_position-50:error_position+50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AI_ACT-with-image-refs...\n",
      "Failed to parse JSON even after cleaning: Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
      "Processing Cybersecurity_California_Privacy-with-image-refs...\n",
      "Processing GDPR-with-image-refs...\n",
      "Saved 15 questions to evaluation_sets.json\n"
     ]
    }
   ],
   "source": [
    "# def clean_and_parse_eval_set(eval_set_str: str) -> list:\n",
    "#     \"\"\"Clean and parse the eval set string into a list of dictionaries.\"\"\"\n",
    "#     try:\n",
    "#         # First attempt: direct parsing\n",
    "#         return json.loads(eval_set_str)\n",
    "#     except json.JSONDecodeError:\n",
    "#         try:\n",
    "#             # Second attempt: fix common issues\n",
    "#             # Replace any unescaped quotes within the text\n",
    "#             cleaned = eval_set_str.replace('\\\\\"', '\"')  # First unescape any escaped quotes\n",
    "#             cleaned = cleaned.replace('\"', '\\\\\"')       # Then escape all quotes\n",
    "#             cleaned = cleaned.replace('\\\\\"{\"', '{\"')    # Fix the start of each object\n",
    "#             cleaned = cleaned.replace('\"}\\\\\"', '\"}')    # Fix the end of each object\n",
    "#             # Ensure the string starts and ends with square brackets\n",
    "#             if not cleaned.startswith('['):\n",
    "#                 cleaned = '[' + cleaned\n",
    "#             if not cleaned.endswith(']'):\n",
    "#                 cleaned = cleaned + ']'\n",
    "#             return json.loads(cleaned)\n",
    "#         except json.JSONDecodeError as e:\n",
    "#             print(f\"Failed to parse JSON even after cleaning: {str(e)}\")\n",
    "#             return []\n",
    "\n",
    "# # Try parsing the eval sets with the new function\n",
    "# parsed_eval_sets = []\n",
    "\n",
    "# for doc_id, eval_set in eval_sets.items():\n",
    "#     print(f\"Processing {doc_id}...\")\n",
    "#     questions = clean_and_parse_eval_set(eval_set)\n",
    "    \n",
    "#     # Add document information to each question\n",
    "#     for question in questions:\n",
    "#         question['document'] = doc_id\n",
    "#         parsed_eval_sets.append(question)\n",
    "\n",
    "# # Save to JSON file\n",
    "# output_path = \"evaluation_sets.json\"\n",
    "# with open(output_path, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(parsed_eval_sets, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# print(f\"Saved {len(parsed_eval_sets)} questions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 28 questions to evaluation_sets.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Parse the eval sets and add document information\n",
    "parsed_eval_sets = []\n",
    "\n",
    "for doc_id, eval_set in eval_sets.items():\n",
    "    # Convert string response to Python list of dictionaries\n",
    "    questions = json.loads(eval_set)\n",
    "    \n",
    "    # Add document information to each question\n",
    "    for question in questions:\n",
    "        question['document'] = doc_id\n",
    "        parsed_eval_sets.append(question)\n",
    "\n",
    "# Save to JSON file\n",
    "output_path = \"evaluation_sets.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(parsed_eval_sets, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"Saved {len(parsed_eval_sets)} questions to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the eval sets when needed\n",
    "with open(\"evaluation_sets.json\", 'r', encoding='utf-8') as f:\n",
    "    eval_set = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shres\\anaconda3\\envs\\rag_case_study\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating question 1/6482\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 75\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, question \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(eval_set):\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating question \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(eval_set)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_question\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     76\u001b[0m     evaluation_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# Save results\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 22\u001b[0m, in \u001b[0;36mevaluate_question\u001b[1;34m(question, chunks_data)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03mEvaluate a single question against all document chunks and find top 10 chunks.\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Dictionary containing evaluation results with simplified structure\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Create query-document pairs\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[43mquestion\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     23\u001b[0m sentence_combinations \u001b[38;5;241m=\u001b[39m [[query, chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunk_content\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunks_data]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Get cross-encoder scores\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "# Creating corpus and evaluating query, doc pair using cross-encoders\n",
    "import numpy as np\n",
    "\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "# Pre-trained cross encoder\n",
    "model = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating question 1/28\n",
      "Evaluating question 2/28\n",
      "Evaluating question 3/28\n",
      "Evaluating question 4/28\n",
      "Evaluating question 5/28\n"
     ]
    }
   ],
   "source": [
    "def evaluate_question(question: dict, chunks_data: list) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluate a single question against all document chunks and find top 10 chunks.\n",
    "    \n",
    "    Args:\n",
    "        question: Dictionary containing question data\n",
    "        chunks_data: List of all document chunks\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary containing evaluation results with simplified structure\n",
    "    \"\"\"\n",
    "    # Create query-document pairs\n",
    "    query = question['question']\n",
    "    sentence_combinations = [[query, chunk['chunk_content']] for chunk in chunks_data]\n",
    "    \n",
    "    # Get cross-encoder scores\n",
    "    scores = model.predict(sentence_combinations)\n",
    "    \n",
    "    # Create results list with scores and metadata\n",
    "    results = []\n",
    "    for chunk, score in zip(chunks_data, scores):\n",
    "        results.append({\n",
    "            'chunk_id': chunk['chunk_id'],\n",
    "            'score': float(score)\n",
    "        })\n",
    "    \n",
    "    # Sort results by score in descending order\n",
    "    results.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Get top 10 chunks from cross-encoder\n",
    "    top_10_chunk_ids = [chunk['chunk_id'] for chunk in results[:10]]\n",
    "    \n",
    "    # Find overlapping chunks\n",
    "    overlapping_chunks = [\n",
    "        chunk_id for chunk_id in question['chunk_ids'] \n",
    "        if chunk_id in top_10_chunk_ids\n",
    "    ]\n",
    "    \n",
    "    # Create final ground truth set:\n",
    "    # 1. First add all overlapping chunks\n",
    "    final_ground_truth = overlapping_chunks.copy()\n",
    "    \n",
    "    # 2. Add remaining chunks from top 10 until we have 10 total\n",
    "    remaining_slots = 10 - len(final_ground_truth)\n",
    "    if remaining_slots > 0:\n",
    "        for chunk_id in top_10_chunk_ids:\n",
    "            if chunk_id not in final_ground_truth:\n",
    "                final_ground_truth.append(chunk_id)\n",
    "                remaining_slots -= 1\n",
    "                if remaining_slots == 0:\n",
    "                    break\n",
    "    \n",
    "    return {\n",
    "        'question': question['question'],\n",
    "        'llm_chunk_labels': question['chunk_ids'],\n",
    "        'cross_encoder_top_10': top_10_chunk_ids,\n",
    "        'final_ground_truth': final_ground_truth,\n",
    "        'total_overlap_chunks': len(overlapping_chunks)\n",
    "    }\n",
    "\n",
    "# Evaluate all questions\n",
    "evaluation_results = []\n",
    "\n",
    "for i, question in enumerate(eval_set):\n",
    "    print(f\"Evaluating question {i+1}/{len(eval_set)}\")\n",
    "    result = evaluate_question(question, chunks_data)\n",
    "    evaluation_results.append(result)\n",
    "\n",
    "# Save results\n",
    "output_path = \"cross_encoder_evaluations.json\"\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(evaluation_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"\\nSaved evaluation results to {output_path}\")\n",
    "\n",
    "# Print sample results for the first question\n",
    "if evaluation_results:\n",
    "    first_eval = evaluation_results[0]\n",
    "    print(\"\\nSample evaluation for first question:\")\n",
    "    print(json.dumps(first_eval, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mresult\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
