{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "\n",
    "import vertexai\n",
    "from vertexai.preview.generative_models import (\n",
    "    FunctionDeclaration,\n",
    "    GenerativeModel,\n",
    "    Tool,\n",
    "    ToolConfig,\n",
    "    Part,\n",
    "    GenerationConfig,\n",
    ")\n",
    "PROJECT_ID = \"104916006626\"  # @param {type: \"string\", placeholder: \"[your-project-id]\" isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"xyz\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"australia-southeast1\")\n",
    "\n",
    "vertexai.init(project=\"104916006626\", location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Path to your service account key file\n",
    "key_path = \"C:\\\\Users\\\\shres\\\\Projects\\\\RAG-case-study\\keys\\\\keyproject-401005-6e1cdcbb5996.json\"\n",
    "\n",
    "# Create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path\n",
    ")\n",
    "\n",
    "# Set the credentials for the current environment\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "# auth_request = transport.requests.Request()\n",
    "# credentials.refresh(auth_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompts import system_prompt_QA_eval_bot\n",
    "# Controlled generation example test\n",
    "response_schema = {\n",
    "    \"type\": \"array\",\n",
    "    \"items\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "            },\n",
    "            \"difficulty\": {\n",
    "                \"type\": \"string\",\n",
    "                \"enum\": [\"easy\", \"medium\", \"hard\"],\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\", \"difficulty\"],\n",
    "    },\n",
    "\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(context, num_questions=10):\n",
    "    \"\"\"\n",
    "    Generate a set of questions and answers from a given context.\n",
    "\n",
    "    Args:\n",
    "    context: The context to generate questions from.\n",
    "    num_questions: The number of questions to generate.\n",
    "\n",
    "    Returns:\n",
    "    A list of questions and answers.\n",
    "    \"\"\"\n",
    "    model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "\n",
    "    response = model.generate_content(\n",
    "    \"List a few popular cookie recipes\",\n",
    "    generation_config=GenerationConfig(\n",
    "        response_mime_type=\"application/json\", response_schema=response_schema\n",
    "    ),\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a cross encode to get the similarity between the eval question and the document chunks\n",
    " The cross encoder will output a score between 0 and 1 for each question and document chunk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shres\\anaconda3\\envs\\rag_case_study\\lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\shres\\.cache\\huggingface\\hub\\models--cross-encoder--stsb-distilroberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "model = CrossEncoder(\"cross-encoder/stsb-distilroberta-base\")\n",
    "scores = model.predict([[\"My first\", \"sentence pair\"], [\"Second text\", \"pair\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11382268, 0.11522377], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "from transformers import AutoTokenizer\n",
    "from docling.chunking import HybridChunker\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "class DocumentChunker:\n",
    "    def __init__(self, base_dir: str = \"processed_docs\", model_id: str = \"answerdotai/ModernBERT-base\"):\n",
    "        \"\"\"\n",
    "        Initialize the DocumentChunker with necessary components.\n",
    "        \n",
    "        Args:\n",
    "            base_dir: Base directory containing markdown files\n",
    "            model_id: Model ID for the tokenizer\n",
    "        \"\"\"\n",
    "        # Setup logging\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.model_id = model_id\n",
    "        \n",
    "        # Initialize components\n",
    "        self._setup_components()\n",
    "        \n",
    "        # Store results\n",
    "        self.document_chunks: Dict[str, List[str]] = {}\n",
    "\n",
    "    def _setup_components(self) -> None:\n",
    "        \"\"\"Initialize tokenizer, chunker and document converter.\"\"\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n",
    "        self.chunker = HybridChunker(\n",
    "            tokenizer=self.tokenizer,\n",
    "            merge_peers=True,\n",
    "        )\n",
    "        self.doc_converter = DocumentConverter()\n",
    "        \n",
    "    def process_single_document(self, file_path: Path) -> List[str]:\n",
    "        \"\"\"\n",
    "        Process a single markdown file and return its chunks.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to the markdown file\n",
    "            \n",
    "        Returns:\n",
    "            List of chunks for the document\n",
    "        \"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        try:\n",
    "            # Convert markdown to docling document\n",
    "            doc = self.doc_converter.convert(source=str(file_path)).document\n",
    "            \n",
    "            # Generate and store chunks in order\n",
    "            for chunk in self.chunker.chunk(dl_doc=doc):\n",
    "                chunks.append(self.chunker.serialize(chunk=chunk))\n",
    "                \n",
    "            self.logger.info(f\"Successfully processed {file_path.name} - Generated {len(chunks)} chunks\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error processing {file_path.name}: {str(e)}\")\n",
    "        \n",
    "        return chunks\n",
    "\n",
    "    def process_directory(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Process all markdown files in the directory and its subdirectories.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping document names to their ordered chunks\n",
    "        \"\"\"\n",
    "        # Find all markdown files\n",
    "        md_files = list(self.base_dir.glob(\"**/*-with-image-refs.md\"))\n",
    "        \n",
    "        if not md_files:\n",
    "            self.logger.warning(f\"No markdown files found in {self.base_dir}\")\n",
    "            return self.document_chunks\n",
    "        \n",
    "        self.logger.info(f\"Found {len(md_files)} markdown files to process\")\n",
    "        \n",
    "        # Process each file\n",
    "        for md_file in md_files:\n",
    "            self.logger.info(f\"Processing {md_file.relative_to(self.base_dir)}\")\n",
    "            \n",
    "            # Store chunks with document name as key\n",
    "            doc_key = md_file.stem\n",
    "            self.document_chunks[doc_key] = self.process_single_document(md_file)\n",
    "        \n",
    "        self.logger.info(f\"Completed processing all documents\")\n",
    "        return self.document_chunks\n",
    "    \n",
    "    def get_document_statistics(self) -> None:\n",
    "        \"\"\"Print statistics about processed documents and their chunks.\"\"\"\n",
    "        if not self.document_chunks:\n",
    "            print(\"No documents have been processed yet.\")\n",
    "            return\n",
    "            \n",
    "        print(\"\\nDocument Processing Statistics:\")\n",
    "        print(\"-\" * 30)\n",
    "        for doc_name, chunks in self.document_chunks.items():\n",
    "            print(f\"\\nDocument: {doc_name}\")\n",
    "            print(f\"Number of chunks: {len(chunks)}\")\n",
    "            if chunks:\n",
    "                avg_chunk_length = sum(len(self.tokenizer.tokenize(chunk)) \n",
    "                                     for chunk in chunks) / len(chunks)\n",
    "                print(f\"Average chunk length: {avg_chunk_length:.2f} tokens\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the chunker\n",
    "    doc_chunker = DocumentChunker()\n",
    "    \n",
    "    # Process all documents\n",
    "    document_chunks = doc_chunker.process_directory()\n",
    "    \n",
    "    # Print statistics\n",
    "    doc_chunker.get_document_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
