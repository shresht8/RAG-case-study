{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the environment variable if the user doesn't provide Project ID.\n",
    "import os\n",
    "import json\n",
    "import vertexai\n",
    "from google import genai\n",
    "from google.genai.types import GenerateContentConfig, Part\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "PROJECT_ID = os.getenv(\"PROJECT_ID\")  # @param {type: \"string\", placeholder: \"[your-project-id]\" isTemplate: true}\n",
    "if not PROJECT_ID or PROJECT_ID == \"xyz\":\n",
    "    PROJECT_ID = str(os.environ.get(\"GOOGLE_CLOUD_PROJECT\"))\n",
    "\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"australia-southeast1\")\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)\n",
    "\n",
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 20 pages (1 to 20) to .\\processed_docs\\Privacy_Act_AU\\Privacy_Act_AU_extracted.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'.\\\\processed_docs\\\\Privacy_Act_AU\\\\Privacy_Act_AU_extracted.pdf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import save_pdf_pages\n",
    "save_pdf_pages(\".\\\\processed_docs\\\\Privacy_Act_AU\\\\Privacy_Act_AU.pdf\", \n",
    "               1, \n",
    "               20, \n",
    "               \".\\\\processed_docs\\\\Privacy_Act_AU\\\\Privacy_Act_AU_extracted.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# Path to your service account key file\n",
    "key_path = \"C:\\\\Users\\\\shres\\\\Projects\\\\RAG-case-study\\keys\\\\keyproject-401005-6e1cdcbb5996.json\"\n",
    "\n",
    "# Create credentials using the service account key file\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path\n",
    ")\n",
    "\n",
    "# Set the credentials for the current environment\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "# auth_request = transport.requests.Request()\n",
    "# credentials.refresh(auth_request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "PDF_MIME_TYPE = \"application/pdf\"\n",
    "JSON_MIME_TYPE = \"application/json\"\n",
    "ENUM_MIME_TYPE = \"text/x.enum\"\n",
    "MODEL_ID = \"gemini-1.5-pro-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "class Section(BaseModel):\n",
    "    title: str = Field(description=\"The title of the section\")\n",
    "    page_number: int = Field(description=\"The page number of the section\")\n",
    "    section_number: str = Field(description=\"The section number of the section\")\n",
    "\n",
    "class SubDivision(BaseModel):\n",
    "    title: str = Field(description=\"The title of the sub-division\")\n",
    "    sections: List[Section] | None = Field(description=\"The sections of the sub-division\")\n",
    "\n",
    "class Division(BaseModel):\n",
    "    title: str = Field(description=\"The title of the division\")\n",
    "    sub_divisions: List[SubDivision] | None = Field(description=\"The sub-divisions of the division\")\n",
    "    sections: List[Section] | None = Field(description=\"The sections of the division\")\n",
    "\n",
    "class Part_TOC(BaseModel):\n",
    "    title: str = Field(description=\"The title of the part\")\n",
    "    divisions: List[Division] | None = Field(description=\"The divisions of the part\")\n",
    "    sections: List[Section] | None = Field(description=\"The sections of the part\")\n",
    "\n",
    "class TableOfContents(BaseModel):\n",
    "    parts: List[Part_TOC] = Field(description=\"The parts of the table of contents\")\n",
    "\n",
    "class Response(BaseModel):\n",
    "    table_of_contents: TableOfContents = Field(description=\"The table of contents of the document\")\n",
    "    is_finished: bool = Field(description=\"Whether the table of contents has been extracted fully\")\n",
    "\n",
    "class ResponsePart(BaseModel):\n",
    "    part: Part_TOC = Field(description=\"The part of the table of contents\")\n",
    "    is_part_extracted: bool = Field(description=\"Whether the part has been extracted fully\")\n",
    "    part_extracted: str = Field(description=\"Title of the part that has been extracted\")\n",
    "    next_part_title: str | None = Field(description=\"Title of the next part of the table of contents to be extracted\")\n",
    "    is_toc_finished: bool = Field(description=\"Whether the table of contents has been extracted fully.\"\n",
    "                                  \"Yes if the part is the last part of the table of contents, otherwise no\")\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msectionTOC\u001b[39;00m(\u001b[43mBaseModel\u001b[49m):\n\u001b[0;32m      2\u001b[0m     title: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe title of the section\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m     page_number: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m Field(description\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe page number of the section\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseModel' is not defined"
     ]
    }
   ],
   "source": [
    "class sectionTOC(BaseModel):\n",
    "    title: str = Field(description=\"The title of the section\")\n",
    "    page_number: int = Field(description=\"The page number of the section\")\n",
    "\n",
    "class ccpaTOC(BaseModel):\n",
    "    main_heading: str | None = Field(description=\"The title of the main heading\")\n",
    "    section: List[sectionTOC] | None = Field(description=\"The sections within the heading\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_PAU = \"\"\"\n",
    "You are an intelligent agent who extracts table of contents from documents part by part.\n",
    "Your task is to:\n",
    "1. Extract ONLY ONE PART at a time from the table of contents\n",
    "2. If no previous extraction exists, start with the first part\n",
    "3. If previous parts were extracted, extract the next part as indicated\n",
    "4. Set is_part_extracted to true when the current part is fully extracted\n",
    "5. Set part_extracted to the title of the current part you've extracted\n",
    "6. Set next_part_title to the title of the next part to be extracted (null if this is the last part)\n",
    "7. Set is_toc_finished to true ONLY if this is the last part of the table of contents\n",
    "\n",
    "Important:\n",
    "- Extract only ONE part per response\n",
    "- Be thorough and accurate in extraction\n",
    "- Maintain proper structure (part -> divisions -> subdivisions -> sections)\n",
    "- Include all page numbers and section numbers\n",
    "- If you see the next part title in the document, include it in next_part_title\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vertex_ai(project_id: str, location: str):\n",
    "    \"\"\"Initialize Vertex AI client\"\"\"\n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    return genai.Client(vertexai=True, project=project_id, location=location)\n",
    "\n",
    "def extract_next_part(\n",
    "    client: genai.Client,\n",
    "    file_path: str,\n",
    "    current_toc: dict,\n",
    "    system_prompt: str,\n",
    "    next_part_title: Optional[str] = None,\n",
    "    is_markdown: bool = False\n",
    ") -> ResponsePart:\n",
    "    \"\"\"\n",
    "    Extract the next part of the table of contents from either PDF or markdown\n",
    "    \n",
    "    Args:\n",
    "        client: Initialized Vertex AI client\n",
    "        file_path: Path to PDF or markdown file\n",
    "        current_toc: Current state of the table of contents\n",
    "        next_part_title: Title of the part to extract (if known)\n",
    "        is_markdown: Boolean indicating if the input file is markdown\n",
    "    \n",
    "    Returns:\n",
    "        ResponsePart containing the extracted part and metadata\n",
    "    \"\"\"\n",
    "    print(\"starting extraction...\")\n",
    "    \n",
    "    # Read file based on type\n",
    "    if is_markdown:\n",
    "        with open(file_path, \"r\", encoding='utf-8') as f:\n",
    "            file_content = f.read()\n",
    "        # For markdown, add content directly as string\n",
    "        contents = [\n",
    "            \"Extract the next part of the table of contents.\",\n",
    "            file_content\n",
    "        ]\n",
    "    else:\n",
    "        # For PDF, use Part.from_bytes as before\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            file_bytes = f.read()\n",
    "        contents = [\n",
    "            \"Extract the next part of the table of contents.\",\n",
    "            Part.from_bytes(data=file_bytes, mime_type=PDF_MIME_TYPE),\n",
    "        ]\n",
    "    \n",
    "    print(\"file read...\")\n",
    "    \n",
    "    # If we have current TOC and next part title, include them\n",
    "    if current_toc and len(current_toc[\"parts\"]) > 0:\n",
    "        contents.insert(1, f\"Previously extracted parts: {json.dumps(current_toc)}\")\n",
    "    if next_part_title:\n",
    "        contents.insert(1, f\"Please extract the part titled: {next_part_title}\")\n",
    "    \n",
    "    print(\"contents prepared...\")\n",
    "    \n",
    "    # Generate content using LLM\n",
    "    response = client.models.generate_content(\n",
    "        model=MODEL_ID,\n",
    "        contents=contents,\n",
    "        config=GenerateContentConfig(\n",
    "            system_instruction=system_prompt,\n",
    "            temperature=0,\n",
    "            response_schema=ResponsePart,\n",
    "            response_mime_type=JSON_MIME_TYPE,\n",
    "        ),\n",
    "    )\n",
    "    print(\"response generated...\")\n",
    "    \n",
    "    # Parse and validate response\n",
    "    result = json.loads(response.text)\n",
    "    return ResponsePart(**result)\n",
    "\n",
    "def extract_full_toc(\n",
    "    client: genai.Client,\n",
    "    file_path: str,\n",
    "    max_parts: int = 20,\n",
    "    is_markdown: bool = False\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Extract complete table of contents part by part\n",
    "    \n",
    "    Args:\n",
    "        client: Initialized Vertex AI client\n",
    "        file_path: Path to PDF or markdown file\n",
    "        max_parts: Maximum number of parts to prevent infinite loops\n",
    "        is_markdown: Boolean indicating if the input file is markdown\n",
    "    \n",
    "    Returns:\n",
    "        Complete table of contents as dictionary\n",
    "    \"\"\"\n",
    "    # Initialize empty TOC\n",
    "    toc = {\"parts\": []}\n",
    "    next_part_title = None\n",
    "    parts_extracted = 0\n",
    "    print(\"initializing TOC...\")\n",
    "    \n",
    "    while parts_extracted < max_parts:\n",
    "        print(\"extracting next part...\")\n",
    "        # Extract next part\n",
    "        response = extract_next_part(client, file_path, toc, SYSTEM_PROMPT_PAU, next_part_title, is_markdown)\n",
    "        \n",
    "        # Add the extracted part to our TOC\n",
    "        toc[\"parts\"].append(response.part.model_dump())\n",
    "        parts_extracted += 1\n",
    "\n",
    "        # Save intermediate result\n",
    "        with open(\"Experiments\\\\002\\\\toc_data_intermediate.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(toc, f, indent=4)\n",
    "\n",
    "        print(f\"Extracted part: {response.part_extracted}\")\n",
    "        \n",
    "        # Check if we're done\n",
    "        if response.is_toc_finished:\n",
    "            break\n",
    "            \n",
    "        # Update next part to extract\n",
    "        next_part_title = response.next_part_title\n",
    "        \n",
    "        if not next_part_title:\n",
    "            print(\"Warning: No next part title provided but TOC not marked as finished\")\n",
    "            break\n",
    "\n",
    "    return toc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing TOC...\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part I—Preliminary\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part II—Interpretation\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part III—Information privacy\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part IIIA—Credit reporting\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part IIIB—Privacy codes\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part IIIC-Notification of eligible data breaches\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part IV—Functions of the Information Commissioner\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part V—Investigations etc.\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part VI—Public interest determinations and temporary public interest determinations\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part VIA—Dealing with personal information in emergencies and disasters\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part VIB—Compliance and enforcement\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part VII—Privacy Advisory Committee\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part VIII—Obligations of confidence\n",
      "extracting next part...\n",
      "starting extraction...\n",
      "file read...\n",
      "contents prepared...\n",
      "response generated...\n",
      "Extracted part: Part IX—Miscellaneous\n",
      "Complete table of contents has been written to Experiments\\002\\toc_data_full_mistral.json\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Initialize Vertex AI\n",
    "    PROJECT_ID = os.getenv(\"PROJECT_ID\")\n",
    "    LOCATION = \"australia-southeast1\"\n",
    "    client = initialize_vertex_ai(PROJECT_ID, LOCATION)\n",
    "\n",
    "    # Path to PDF\n",
    "    file_path = \".\\\\Experiments\\\\002\\\\toc_data_full.md\"\n",
    "\n",
    "    try:\n",
    "        # Extract table of contents\n",
    "        toc = extract_full_toc(client, file_path, is_markdown=True)\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(\"Experiments\\\\002\", exist_ok=True)\n",
    "\n",
    "        # Write final result to file\n",
    "        output_file = \"Experiments\\\\002\\\\toc_data_full_mistral.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(toc, f, indent=4)\n",
    "\n",
    "        print(f\"Complete table of contents has been written to {output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed and saved Experiments/002/toc_data_full_mistral.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from utils import fix_unicode_characters\n",
    "# Usage\n",
    "json_path = \"Experiments/002/toc_data_full_mistral.json\"\n",
    "fix_unicode_characters(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "- Experiment 1: Extracting structured JSON response of table of contents of Privancy_Act_AU.pdf from the PDF directly via gemini\n",
    "- Experiment 2: Using Mistral OCR to parse Table of Contents to markdown + Using markdown as input to extract structured JSON via gemini\n",
    "\n",
    "### Observations:\n",
    "- Experiment 1 has some drawbacks when there are longer and convoluted Parts to extract\n",
    "- Experiment 2 solves those problems with a more optimal OCR to Markdown converter. Gemini then accurately extracts JSON from the markdown of the table of contents\n",
    "- Markdown to JSON is more accurate compared to PDF to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Extracting structured output using Mistral OCR\n",
    "\n",
    "from mistralai import Mistral\n",
    "import os\n",
    "\n",
    "api_key = os.environ[\"Mistral_API_KEY\"]\n",
    "\n",
    "client = Mistral(api_key=api_key)\n",
    "\n",
    "uploaded_pdf = client.files.upload(\n",
    "    file={\n",
    "        \"file_name\": \"Cybersecurity_California_Privacy.pdf\",\n",
    "        \"content\": open(\".\\\\processed_docs\\\\Cybersecurity_California_Privacy\\\\Cybersecurity_California_Privacy.pdf\", \"rb\"),\n",
    "    },\n",
    "    purpose=\"ocr\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from mistralai import Mistral\n",
    "signed_url = client.files.get_signed_url(file_id=uploaded_pdf.id)\n",
    "api_key = os.environ[\"Mistral_API_KEY\"]\n",
    "client = Mistral(api_key=api_key)\n",
    "ocr_response = client.ocr.process(\n",
    "    model=\"mistral-ocr-latest\",\n",
    "    document={\n",
    "        \"type\": \"document_url\",\n",
    "        \"document_url\": signed_url.url,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_markdown_pages(ocr_response):\n",
    "    \"\"\"\n",
    "    Combines markdown content from multiple pages into a single string\n",
    "    \n",
    "    Args:\n",
    "        ocr_response: Dictionary containing 'pages' key with list of objects having markdown content\n",
    "        \n",
    "    Returns:\n",
    "        str: Combined markdown content from all pages\n",
    "    \"\"\"\n",
    "    combined_markdown = \"\"\n",
    "    \n",
    "    for page in ocr_response.pages:\n",
    "        combined_markdown += page.markdown + \"\\n\\n\"  # Add double newline between pages\n",
    "        \n",
    "    return combined_markdown.strip()  # Remove trailing whitespace\n",
    "\n",
    "# Example usage:\n",
    "# combined_content = combine_markdown_pages(ocr_response)\n",
    "# \n",
    "# # Write to file if needed\n",
    "# with open(\"output.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "#     f.write(combined_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing Table of contents to markdown\n",
    "combined_content = combine_markdown_pages(ocr_response)\n",
    "\n",
    "# Write to file if needed\n",
    "with open(\"Experiments\\\\002\\\\toc_data_full.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(combined_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Writing the document to markdown\n",
    "combined_content = combine_markdown_pages(ocr_response)\n",
    "\n",
    "# Write to file if needed\n",
    "with open(\".\\\\processed_docs\\\\Cybersecurity_California_Privacy\\\\Cybersecurity_California_Privacy_extracted.md\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(combined_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
