{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG System\n",
    "Now that the data pre-processing, indexing and evaluation for retrieval is complete the next step is to put it all together and create the RAG system which works end to end. This notebook will include the following:\n",
    "1. Read chunks from the directory of the current run (experiment).\n",
    "2. Create index with the chunks and the chunk ids\n",
    "3. Link the retriever and LLM together to create an end to end pipeline where the user asks questions and receives an answer from the LLM which includes the answer as well as the sources for the answer\n",
    "4. Input and output validation and guardrails to prevent LLM from hallucinating, leaking PII etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from pydantic import BaseModel, Field, field_validator, ValidationInfo\n",
    "from typing import Optional, Dict, Any, List, Annotated\n",
    "from dataclasses import dataclass\n",
    "import instructor\n",
    "from instructor import openai_moderation\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "client = instructor.from_openai(OpenAI(api_key=OPENAI_API_KEY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating a retriever class\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "from pylate import indexes, models, retrieve\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self, experiment_number: str):\n",
    "        \"\"\"Initialize the Retriever with experiment number.\n",
    "        \n",
    "        Args:\n",
    "            experiment_number (str): The experiment number (e.g., '001')\n",
    "        \"\"\"\n",
    "        self.experiment_dir = Path(f\"Experiments/{experiment_number}\")\n",
    "        self.model = models.ColBERT(\n",
    "            model_name_or_path=\"shresht8/modernBERT_text_similarity_finetune\"\n",
    "        )\n",
    "        self.index = indexes.Voyager(\n",
    "            index_folder=\"pylate-index\",\n",
    "            index_name=\"index\",\n",
    "            override=True\n",
    "        )\n",
    "        self.retriever = None\n",
    "        self.chunks_data = None\n",
    "        \n",
    "    def read_chunks(self) -> List[Dict]:\n",
    "        \"\"\"Read document chunks from the experiment directory.\"\"\"\n",
    "        chunks_path = self.experiment_dir / \"document_chunks.json\"\n",
    "        with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "            self.chunks_data = json.load(f)\n",
    "        return self.chunks_data\n",
    "    \n",
    "    def create_index(self):\n",
    "        \"\"\"Create index from chunks and initialize retriever.\"\"\"\n",
    "        if self.chunks_data is None:\n",
    "            self.read_chunks()\n",
    "            \n",
    "        # Prepare lists for indexing\n",
    "        all_chunks = []\n",
    "        chunk_ids = []\n",
    "        \n",
    "        # Extract chunks and their IDs\n",
    "        for chunk in self.chunks_data:\n",
    "            all_chunks.append(chunk['chunk_content'])\n",
    "            chunk_ids.append(chunk['chunk_id'])\n",
    "            \n",
    "        # Encode all chunks\n",
    "        documents_embeddings = self.model.encode(\n",
    "            all_chunks,\n",
    "            batch_size=32,\n",
    "            is_query=False,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        # Add documents to index\n",
    "        self.index.add_documents(\n",
    "            documents_ids=chunk_ids,\n",
    "            documents_embeddings=documents_embeddings\n",
    "        )\n",
    "        \n",
    "        # Initialize retriever\n",
    "        self.retriever = retrieve.ColBERT(index=self.index)\n",
    "        \n",
    "    def get_relevant_chunks(self, query: str, k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Retrieve relevant chunks for a given query.\n",
    "        \n",
    "        Args:\n",
    "            query (str): The search query\n",
    "            k (int): Number of chunks to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of relevant chunks with their metadata\n",
    "        \"\"\"\n",
    "        if self.retriever is None:\n",
    "            raise ValueError(\"Index not created. Call create_index() first.\")\n",
    "            \n",
    "        # Encode the query\n",
    "        query_embeddings = self.model.encode(\n",
    "            [query],\n",
    "            batch_size=32,\n",
    "            is_query=True,\n",
    "            show_progress_bar=False\n",
    "        )\n",
    "        \n",
    "        # Get top k retrievals\n",
    "        scores = self.retriever.retrieve(\n",
    "            queries_embeddings=query_embeddings,\n",
    "            k=k\n",
    "        )\n",
    "        \n",
    "        # Get retrieved chunk IDs\n",
    "        retrieved_chunks = scores[0]  # First (and only) query results\n",
    "        retrieved_chunk_ids = [chunk['id'] for chunk in retrieved_chunks]\n",
    "        \n",
    "        # Map chunk IDs to full chunk data\n",
    "        chunk_map = {chunk['chunk_id']: chunk for chunk in self.chunks_data}\n",
    "        relevant_chunks = [chunk_map[chunk_id] for chunk_id in retrieved_chunk_ids]\n",
    "        \n",
    "        return relevant_chunks\n",
    "    \n",
    "    def format_chunks_to_context(self, chunks: List[Dict]) -> str:\n",
    "        \"\"\"Format retrieved chunks into a single context string.\n",
    "        \n",
    "        Args:\n",
    "            chunks (List[Dict]): List of chunk dictionaries\n",
    "            \n",
    "        Returns:\n",
    "            str: Formatted context string with source information\n",
    "        \"\"\"\n",
    "        formatted_chunks = []\n",
    "        for chunk in chunks:\n",
    "            chunk_text = (\n",
    "                f\"Source: {chunk['document_name']}\\n\"\n",
    "                f\"Content: {chunk['chunk_content']}\\n\"\n",
    "            )\n",
    "            formatted_chunks.append(chunk_text)\n",
    "            \n",
    "        return \"\\n\".join(formatted_chunks)\n",
    "    \n",
    "    def create_prompt(self, query: str, system_prompt: str, k: int = 3) -> List[Dict]:\n",
    "        \"\"\"Create a formatted prompt for the OpenAI API with retrieved context.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's question\n",
    "            system_prompt (str): System prompt for the LLM\n",
    "            k (int): Number of chunks to retrieve\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: Formatted messages for the OpenAI API\n",
    "        \"\"\"\n",
    "        # Get relevant chunks\n",
    "        relevant_chunks = self.get_relevant_chunks(query, k=k)\n",
    "        \n",
    "        # Format chunks into context\n",
    "        context = self.format_chunks_to_context(relevant_chunks)\n",
    "        \n",
    "        # Create messages array for OpenAI API\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyLate model loaded successfully.\n",
      "Encoding documents (bs=32): 100%|██████████| 10/10 [01:55<00:00, 11.59s/it]\n",
      "Adding documents to the index (bs=2000): 100%|██████████| 1/1 [00:06<00:00,  6.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "retriever = Retriever(experiment_number=\"001\")\n",
    "retriever.create_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrieving documents (bs=50): 100%|██████████| 1/1 [00:02<00:00,  2.19s/it]\n"
     ]
    }
   ],
   "source": [
    "class AnswerWithCitation(BaseModel):\n",
    "    \"\"\"Validates and structures the final response. Answers are provided with citation to keep the response to user query is\n",
    "    grounded to context provided by user, prevent harmful responses and maxise accuracy of response\"\"\"\n",
    "    answer: Annotated[\n",
    "        str, \n",
    "        Field(description='Final answer to user. Must be a response that is relevant to the user query if relevant information is available. Otherwise the assistant cannot help user'),\n",
    "        openai_moderation(client=client)\n",
    "    ] = Field(...)\n",
    "    \n",
    "    citation: Optional[str] = Field(\n",
    "        None,\n",
    "        description='Citation from the context provided. Can be a sentence from the context provided or it can be the entire context. If not relevant context is available it must be None'\n",
    "    )\n",
    "\n",
    "# Define system prompt\n",
    "system_prompt = \"You are a helpful assistant. Answer the question based on the provided context only. If you cannot find the answer in the context, say so.\"\n",
    "\n",
    "# Create formatted prompt\n",
    "query = \"What are the requirements for AI systems?\"\n",
    "messages = retriever.create_prompt(\n",
    "    query=query,\n",
    "    system_prompt=system_prompt,\n",
    "    k=10\n",
    ")\n",
    "\n",
    "# Use with OpenAI API\n",
    "response = client.chat.completions.create(\n",
    "    response_model=AnswerWithCitation,\n",
    "    model=\"o1-2024-12-17\",\n",
    "    messages=messages,\n",
    "    max_retries=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer='Under the proposal reflected in the provided context, all “high-risk” AI systems must satisfy several core requirements intended to safeguard health, safety, and fundamental rights. In particular, providers must: • Establish a risk-management system that identifies and addresses known and foreseeable risks in light of the system’s intended purpose and reasonably predictable misuse; • Use data sets that are sufficiently relevant, representative, accurate, and complete for their intended purpose and that mitigate potential bias; • Maintain up-to-date technical documentation that demonstrates compliance with these regulatory obligations; • Log the system’s operations in order to support traceability and facilitate oversight by regulators; • Provide clear, adequate instructions and transparency information so deployers understand how to use the AI system safely and reliably; • Ensure human oversight mechanisms are in place, so that humans can monitor how the system functions and intervene if necessary; • Achieve appropriate levels of accuracy, robustness, and cybersecurity to protect the system from vulnerabilities; • Complete conformity assessments demonstrating compliance, affix the CE marking if applicable, and register high-risk AI systems in a central EU database before market placement. These requirements build on the overall aim of ensuring that AI systems (especially those designated high-risk) operate in a way that upholds fundamental rights and public trust, and that providers and other operators put in place clear safeguards such as risk analysis, thorough documentation, and oversight measures.' citation='See Source: AI_ACT-with-image-refs, in particular “Compliance with the requirements laid down in this Section …” and “Obligations of providers of high-risk AI systems”'\n"
     ]
    }
   ],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_case_study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
